{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60058b41",
   "metadata": {},
   "source": [
    "# IBM Model 1 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d92f93",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa83535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "DATASET_FILE = \"../dataset/ita.txt\"\n",
    "\n",
    "def clean_sentence(sentence : str) -> str:\n",
    "    mapping = str.maketrans(\"\", \"\", \".!'\\\",?\")\n",
    "    return sentence.translate(mapping)\n",
    "\n",
    "def read_dataset() -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads a parallel dataset from a specified file.\n",
    "\n",
    "    Assumes the file contains tab-separated English and Italian sentences,\n",
    "    with English in the first column and Italian in the second.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains an (English sentence, Italian sentence) pair.\n",
    "        Returns an empty list if the file is not found or is empty.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    if not os.path.exists(DATASET_FILE):\n",
    "        print(f\"Error: Dataset file not found at {DATASET_FILE}\")\n",
    "        return dataset\n",
    "\n",
    "    try:\n",
    "        with open(DATASET_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Strip whitespace and split by tab\n",
    "                parts = line.strip().split('\\t')\n",
    "                dataset.append((clean_sentence(parts[0]), clean_sentence(parts[1])))\n",
    "        print(f\"Successfully loaded {len(dataset)} sentence pairs from {DATASET_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading dataset file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77271f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "613f9fc4",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedadf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Set\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "class IBMModel1():\n",
    "    \"\"\"\n",
    "    IBM Model 1 for statistical machine translation.\n",
    "    \n",
    "    This implementation follows the EM algorithm for learning word alignment\n",
    "    probabilities between source and target languages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class constants\n",
    "    TRAINING_DATA_PERCENTAGE = 0.8\n",
    "    SENTENCE_LIMIT = 150_000\n",
    "    NUM_ITERATIONS = 10\n",
    "    EPSILON = 1e-12\n",
    "    CHECKPOINT_FILE = \"ibm_model1_checkpoint.npz\"\n",
    "    \n",
    "    def __init__(self, seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the IBM Model 1.\n",
    "        \n",
    "        Args:\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Load and prepare dataset\n",
    "        self.dataset = read_dataset()\n",
    "        random.shuffle(self.dataset)\n",
    "        \n",
    "        # Initialize vocabularies and mappings\n",
    "        self.italian_vocab: Set[str] = set()\n",
    "        self.english_vocab: Set[str] = set()\n",
    "        self.italian_mapping: Dict[str, int] = {}\n",
    "        self.english_mapping: Dict[str, int] = {}\n",
    "        \n",
    "        # Build vocabularies from the dataset\n",
    "        self._build_vocabularies()\n",
    "        \n",
    "        print(f\"Italian unique words: {len(self.italian_vocab)}\")\n",
    "        print(f\"English unique words: {len(self.english_vocab)}\")\n",
    "        \n",
    "        # Initialize translation probabilities uniformly\n",
    "        self.translation_probabilities = self._initialize_translation_probabilities()\n",
    "        self.current_iteration = 0\n",
    "    \n",
    "    def _build_vocabularies(self) -> None:\n",
    "        \"\"\"Build vocabularies and word-to-index mappings from the dataset.\"\"\"\n",
    "        italian_counter = 1\n",
    "        english_counter = 1\n",
    "        \n",
    "        # Process limited number of sentences for vocabulary building\n",
    "        for english_sentence, italian_sentence in self.dataset[:self.SENTENCE_LIMIT]:\n",
    "            # Process English words\n",
    "            for word in english_sentence.split():\n",
    "                if word not in self.english_vocab:\n",
    "                    self.english_mapping[word] = english_counter\n",
    "                    english_counter += 1\n",
    "                    self.english_vocab.add(word)\n",
    "            \n",
    "            # Process Italian words\n",
    "            for word in italian_sentence.split():\n",
    "                if word not in self.italian_vocab:\n",
    "                    self.italian_mapping[word] = italian_counter\n",
    "                    italian_counter += 1\n",
    "                    self.italian_vocab.add(word)\n",
    "    \n",
    "    def _initialize_translation_probabilities(self) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Initialize translation probabilities uniformly.\n",
    "        \n",
    "        Returns:\n",
    "            2D list representing translation probabilities t(f|e)\n",
    "        \"\"\"\n",
    "        english_vocab_size = len(self.english_vocab)\n",
    "        italian_vocab_size = len(self.italian_vocab)\n",
    "        \n",
    "        # Initialize with uniform probabilities\n",
    "        initial_prob = 1.0 / italian_vocab_size\n",
    "        translation_probs = [\n",
    "            [initial_prob] * (italian_vocab_size + 1) \n",
    "            for _ in range(english_vocab_size + 1)\n",
    "        ]\n",
    "        \n",
    "        # Set epsilon values for NULL alignments\n",
    "        for j in self.english_mapping.values():\n",
    "            translation_probs[j][0] = self.EPSILON\n",
    "        for i in self.italian_mapping.values():\n",
    "            translation_probs[0][i] = self.EPSILON\n",
    "        \n",
    "        return translation_probs\n",
    "    \n",
    "    def _get_word_index(self, word: str, mapping: Dict[str, int]) -> int:\n",
    "        \"\"\"Get the index of a word, returning 0 if not found (NULL index).\"\"\"\n",
    "        return mapping.get(word, 0)\n",
    "    \n",
    "    def _compute_alignment_probabilities(self, training_data: List[Tuple[str, str]]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Compute alignment probabilities for the E-step.\n",
    "        \n",
    "        Args:\n",
    "            training_data: List of (English, Italian) sentence pairs\n",
    "            \n",
    "        Returns:\n",
    "            Alignment probabilities for each sentence pair\n",
    "        \"\"\"\n",
    "        alignment_probs = []\n",
    "        total_sentences = len(training_data)\n",
    "        \n",
    "        for i, (english_sentence, italian_sentence) in enumerate(training_data):\n",
    "            english_words = english_sentence.split()\n",
    "            italian_words = italian_sentence.split()\n",
    "            \n",
    "            # Compute normalization factors for each Italian word\n",
    "            normalization_factors = []\n",
    "            for italian_word in italian_words:\n",
    "                it_index = self._get_word_index(italian_word, self.italian_mapping)\n",
    "                total_prob = 0.0\n",
    "                \n",
    "                for english_word in english_words:\n",
    "                    eng_index = self._get_word_index(english_word, self.english_mapping)\n",
    "                    total_prob += self.translation_probabilities[eng_index][it_index]\n",
    "                \n",
    "                normalization_factors.append(total_prob)\n",
    "            \n",
    "            alignment_probs.append(normalization_factors)\n",
    "        \n",
    "        return alignment_probs\n",
    "    \n",
    "    def _update_counts(self, training_data: List[Tuple[str, str]], \n",
    "                      alignment_probs: List[List[float]]) -> Tuple[List[List[float]], List[float]]:\n",
    "        \"\"\"\n",
    "        Update count statistics for the M-step.\n",
    "        \n",
    "        Args:\n",
    "            training_data: List of (English, Italian) sentence pairs\n",
    "            alignment_probs: Precomputed alignment probabilities\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (count matrix, marginal counts)\n",
    "        \"\"\"\n",
    "        \n",
    "        english_vocab_size = len(self.english_vocab) + 1\n",
    "        italian_vocab_size = len(self.italian_vocab) + 1\n",
    "        \n",
    "        # Initialize count matrices\n",
    "        count = [[0.0] * italian_vocab_size for _ in range(english_vocab_size)]\n",
    "        single_count = [self.EPSILON] * english_vocab_size\n",
    "        \n",
    "        total_sentences = len(training_data)\n",
    "        \n",
    "        # Update counts based on alignment probabilities\n",
    "        for k, (english_sentence, italian_sentence) in enumerate(training_data):\n",
    "            english_words = english_sentence.split()\n",
    "            italian_words = italian_sentence.split()\n",
    "            \n",
    "            for i, italian_word in enumerate(italian_words):\n",
    "                it_index = self._get_word_index(italian_word, self.italian_mapping)\n",
    "                \n",
    "                for english_word in english_words:\n",
    "                    eng_index = self._get_word_index(english_word, self.english_mapping)\n",
    "                    \n",
    "                    # Compute alignment probability\n",
    "                    if alignment_probs[k][i] > 0:\n",
    "                        delta = (self.translation_probabilities[eng_index][it_index] / \n",
    "                                alignment_probs[k][i])\n",
    "                        \n",
    "                        # Update counts only for words in vocabulary\n",
    "                        if (english_word in self.english_mapping and \n",
    "                            italian_word in self.italian_mapping):\n",
    "                            count[self.english_mapping[english_word]][self.italian_mapping[italian_word]] += delta\n",
    "                        \n",
    "                        if english_word in self.english_mapping:\n",
    "                            single_count[self.english_mapping[english_word]] += delta\n",
    "        \n",
    "        return count, single_count\n",
    "    \n",
    "    def _update_translation_probabilities(self, count: List[List[float]], \n",
    "                                        single_count: List[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update translation probabilities based on counts.\n",
    "        \n",
    "        Args:\n",
    "            count: Count matrix for word pairs\n",
    "            single_count: Marginal counts for English words\n",
    "        \"\"\"\n",
    "        for i in range(len(count)):\n",
    "            for j in range(len(count[0])):\n",
    "                if single_count[i] > 0:\n",
    "                    self.translation_probabilities[i][j] = max(\n",
    "                        count[i][j] / single_count[i], \n",
    "                        self.EPSILON\n",
    "                    )\n",
    "\n",
    "    def _save_checkpoint(self) -> None:\n",
    "        \"\"\"Saves the current iteration and translation probabilities to a file.\"\"\"\n",
    "        np.savez(self.CHECKPOINT_FILE, \n",
    "                 current_iteration=self.current_iteration, \n",
    "                 translation_probabilities=np.array(self.translation_probabilities),\n",
    "                 english_mapping_keys=list(self.english_mapping.keys()),\n",
    "                 english_mapping_values=list(self.english_mapping.values()),\n",
    "                 italian_mapping_keys=list(self.italian_mapping.keys()),\n",
    "                 italian_mapping_values=list(self.italian_mapping.values()))\n",
    "        print(f\"Checkpoint saved: Iteration {self.current_iteration}, to {self.CHECKPOINT_FILE}\")\n",
    "\n",
    "    def _load_checkpoint(self) -> bool:\n",
    "        \"\"\"\n",
    "        Loads the saved iteration and translation probabilities from a file.\n",
    "        \n",
    "        Returns:\n",
    "            True if a checkpoint was loaded successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.CHECKPOINT_FILE):\n",
    "            print(f\"Loading checkpoint from {self.CHECKPOINT_FILE}...\")\n",
    "            checkpoint_data = np.load(self.CHECKPOINT_FILE, allow_pickle=True)\n",
    "            self.current_iteration = checkpoint_data['current_iteration'].item()\n",
    "            self.translation_probabilities = checkpoint_data['translation_probabilities'].tolist()\n",
    "            \n",
    "            # Reconstruct mappings\n",
    "            self.english_mapping = dict(zip(checkpoint_data['english_mapping_keys'], \n",
    "                                            checkpoint_data['english_mapping_values']))\n",
    "            self.italian_mapping = dict(zip(checkpoint_data['italian_mapping_keys'], \n",
    "                                            checkpoint_data['italian_mapping_values']))\n",
    "            self.english_vocab = set(self.english_mapping.keys())\n",
    "            self.italian_vocab = set(self.italian_mapping.keys())\n",
    "\n",
    "            print(f\"Checkpoint loaded: Starting from iteration {self.current_iteration + 1}\")\n",
    "            return True\n",
    "        print(\"No checkpoint found. Starting training from scratch.\")\n",
    "        return False\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Train the IBM Model 1 using the EM algorithm.\n",
    "        \"\"\"\n",
    "        # Prepare training data\n",
    "        training_size = int(self.TRAINING_DATA_PERCENTAGE * len(self.dataset))\n",
    "        training_data = self.dataset[:training_size]\n",
    "        \n",
    "        print(f\"Training on {len(training_data)} sentence pairs...\")\n",
    "        \n",
    "        # Attempt to load a checkpoint\n",
    "        if self._load_checkpoint():\n",
    "            start_iteration = self.current_iteration + 1\n",
    "        else:\n",
    "            start_iteration = 1\n",
    "        \n",
    "        # EM Algorithm iterations\n",
    "        for iteration in range(start_iteration, self.NUM_ITERATIONS + 1):\n",
    "            self.current_iteration = iteration\n",
    "            print(f\"\\nIteration #{self.current_iteration}\")\n",
    "            \n",
    "            # E-step: Compute alignment probabilities\n",
    "            alignment_probs = self._compute_alignment_probabilities(training_data)\n",
    "            \n",
    "            # M-step: Update counts\n",
    "            count, single_count = self._update_counts(training_data, alignment_probs, show_progress=show_times)\n",
    "            \n",
    "            # M-step: Update probabilities\n",
    "            self._update_translation_probabilities(count, single_count, show_progress=show_times)\n",
    "            \n",
    "            # Save checkpoint after each iteration\n",
    "            self._save_checkpoint()\n",
    "        return\n",
    "    \n",
    "    def translate(self, source_sentence: str) -> str:\n",
    "        \"\"\"\n",
    "        Translate a source sentence using the learned translation probabilities.\n",
    "        \n",
    "        Args:\n",
    "            source_sentence: English sentence to translate\n",
    "            \n",
    "        Returns:\n",
    "            Translated Italian sentence\n",
    "        \"\"\"\n",
    "        english_words = source_sentence.split()\n",
    "        translated_words = []\n",
    "        \n",
    "        for english_word in english_words:\n",
    "            eng_index = self._get_word_index(english_word, self.english_mapping)\n",
    "            \n",
    "            # Find the Italian word with highest translation probability\n",
    "            best_italian_word = None\n",
    "            best_prob = 0.0\n",
    "            \n",
    "            # Skip NULL index (0) and search through all Italian words\n",
    "            for italian_word, it_index in self.italian_mapping.items():\n",
    "                prob = self.translation_probabilities[eng_index][it_index]\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_italian_word = italian_word\n",
    "            \n",
    "            # If no translation found or very low probability, keep the original word\n",
    "            if best_italian_word is None or best_prob < self.EPSILON * 10:\n",
    "                translated_words.append(english_word)\n",
    "            else:\n",
    "                translated_words.append(best_italian_word)\n",
    "        \n",
    "        return ' '.join(translated_words)\n",
    "    \n",
    "    def translate_batch(self, source_sentences: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Translate a batch of sentences.\n",
    "        \n",
    "        Args:\n",
    "            source_sentences: List of English sentences to translate\n",
    "            \n",
    "        Returns:\n",
    "            List of translated Italian sentences\n",
    "        \"\"\"\n",
    "        return [self.translate(sentence) for sentence in source_sentences]\n",
    "    \n",
    "    def get_best_translations(self, english_word: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get the top-k most likely translations for an English word.\n",
    "        \n",
    "        Args:\n",
    "            english_word: English word to translate\n",
    "            top_k: Number of top translations to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (italian_word, probability) tuples, sorted by probability (descending)\n",
    "        \"\"\"\n",
    "        if english_word not in self.english_mapping:\n",
    "            return []\n",
    "        \n",
    "        eng_index = self.english_mapping[english_word]\n",
    "        translations = []\n",
    "        \n",
    "        for italian_word, it_index in self.italian_mapping.items():\n",
    "            prob = self.translation_probabilities[eng_index][it_index]\n",
    "            translations.append((italian_word, prob))\n",
    "        \n",
    "        # Sort by probability (descending) and return top_k\n",
    "        translations.sort(key=lambda x: x[1], reverse=True)\n",
    "        return translations[:top_k]\n",
    "    \n",
    "    def evaluate_bleu(self, test_sentences: List[Tuple[str, str]], verbose: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model using BLEU scores.\n",
    "        \n",
    "        Args:\n",
    "            test_sentences: List of (English, Italian) sentence pairs for evaluation\n",
    "            verbose: Whether to print detailed results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing BLEU scores and other metrics\n",
    "        \"\"\"\n",
    "        if not test_sentences:\n",
    "            raise ValueError(\"No test sentences provided for evaluation\")\n",
    "        \n",
    "        # Prepare data for evaluation\n",
    "        source_sentences = [pair[0] for pair in test_sentences]\n",
    "        reference_sentences = [pair[1] for pair in test_sentences]\n",
    "        \n",
    "        # Generate translations\n",
    "        print(\"Generating translations for evaluation...\")\n",
    "        translated_sentences = self.translate_batch(source_sentences)\n",
    "        \n",
    "        # Prepare references and hypotheses for BLEU calculation\n",
    "        references_tokenized = [[ref.split()] for ref in reference_sentences]\n",
    "        hypotheses_tokenized = [hyp.split() for hyp in translated_sentences]\n",
    "        \n",
    "        # Calculate BLEU scores with smoothing\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        \n",
    "        # Individual sentence BLEU scores\n",
    "        sentence_bleu_scores = []\n",
    "        for i, (ref_tokens, hyp_tokens) in enumerate(zip(references_tokenized, hypotheses_tokenized)):\n",
    "            try:\n",
    "                # Calculate BLEU score for each n-gram level\n",
    "                bleu_1 = sentence_bleu(ref_tokens, hyp_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "                bleu_2 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "                bleu_3 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "                bleu_4 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "                \n",
    "                sentence_bleu_scores.append({\n",
    "                    'bleu_1': bleu_1,\n",
    "                    'bleu_2': bleu_2,\n",
    "                    'bleu_3': bleu_3,\n",
    "                    'bleu_4': bleu_4\n",
    "                })\n",
    "            except ZeroDivisionError:\n",
    "                # Handle edge cases where BLEU cannot be calculated\n",
    "                sentence_bleu_scores.append({\n",
    "                    'bleu_1': 0.0,\n",
    "                    'bleu_2': 0.0,\n",
    "                    'bleu_3': 0.0,\n",
    "                    'bleu_4': 0.0\n",
    "                })\n",
    "        \n",
    "        # Corpus-level BLEU scores\n",
    "        try:\n",
    "            corpus_bleu_1 = corpus_bleu(references_tokenized, hypotheses_tokenized, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "            corpus_bleu_2 = corpus_bleu(references_tokenized, hypotheses_tokenized, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "            corpus_bleu_3 = corpus_bleu(references_tokenized, hypotheses_tokenized, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "            corpus_bleu_4 = corpus_bleu(references_tokenized, hypotheses_tokenized, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "        except ZeroDivisionError:\n",
    "            corpus_bleu_1 = corpus_bleu_2 = corpus_bleu_3 = corpus_bleu_4 = 0.0\n",
    "        \n",
    "        # Calculate average sentence-level BLEU scores\n",
    "        avg_sentence_bleu_1 = sum(score['bleu_1'] for score in sentence_bleu_scores) / len(sentence_bleu_scores)\n",
    "        avg_sentence_bleu_2 = sum(score['bleu_2'] for score in sentence_bleu_scores) / len(sentence_bleu_scores)\n",
    "        avg_sentence_bleu_3 = sum(score['bleu_3'] for score in sentence_bleu_scores) / len(sentence_bleu_scores)\n",
    "        avg_sentence_bleu_4 = sum(score['bleu_4'] for score in sentence_bleu_scores) / len(sentence_bleu_scores)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        total_words_translated = sum(len(hyp.split()) for hyp in translated_sentences)\n",
    "        total_reference_words = sum(len(ref.split()) for ref in reference_sentences)\n",
    "        \n",
    "        # Prepare results\n",
    "        evaluation_results = {\n",
    "            'corpus_bleu_1': corpus_bleu_1,\n",
    "            'corpus_bleu_2': corpus_bleu_2,\n",
    "            'corpus_bleu_3': corpus_bleu_3,\n",
    "            'corpus_bleu_4': corpus_bleu_4,\n",
    "            'avg_sentence_bleu_1': avg_sentence_bleu_1,\n",
    "            'avg_sentence_bleu_2': avg_sentence_bleu_2,\n",
    "            'avg_sentence_bleu_3': avg_sentence_bleu_3,\n",
    "            'avg_sentence_bleu_4': avg_sentence_bleu_4,\n",
    "            'num_test_sentences': len(test_sentences),\n",
    "            'total_words_translated': total_words_translated,\n",
    "            'total_reference_words': total_reference_words,\n",
    "            'avg_sentence_length_translated': total_words_translated / len(test_sentences),\n",
    "            'avg_sentence_length_reference': total_reference_words / len(test_sentences)\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            self._print_evaluation_results(evaluation_results, test_sentences, translated_sentences)\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def _print_evaluation_results(self, results: Dict[str, float], \n",
    "                                 test_sentences: List[Tuple[str, str]], \n",
    "                                 translated_sentences: List[str]) -> None:\n",
    "        \"\"\"Print detailed evaluation results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BLEU SCORE EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nCorpus-level BLEU Scores:\")\n",
    "        print(f\"  BLEU-1: {results['corpus_bleu_1']:.4f}\")\n",
    "        print(f\"  BLEU-2: {results['corpus_bleu_2']:.4f}\")\n",
    "        print(f\"  BLEU-3: {results['corpus_bleu_3']:.4f}\")\n",
    "        print(f\"  BLEU-4: {results['corpus_bleu_4']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nAverage Sentence-level BLEU Scores:\")\n",
    "        print(f\"  BLEU-1: {results['avg_sentence_bleu_1']:.4f}\")\n",
    "        print(f\"  BLEU-2: {results['avg_sentence_bleu_2']:.4f}\")\n",
    "        print(f\"  BLEU-3: {results['avg_sentence_bleu_3']:.4f}\")\n",
    "        print(f\"  BLEU-4: {results['avg_sentence_bleu_4']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nGeneral Statistics:\")\n",
    "        print(f\"  Test sentences: {results['num_test_sentences']}\")\n",
    "        print(f\"  Average translated sentence length: {results['avg_sentence_length_translated']:.2f} words\")\n",
    "        print(f\"  Average reference sentence length: {results['avg_sentence_length_reference']:.2f} words\")\n",
    "        \n",
    "        print(f\"\\nSample Translations:\")\n",
    "        print(\"-\" * 60)\n",
    "        for i in range(min(5, len(test_sentences))):\n",
    "            print(f\"Source:    {test_sentences[i][0]}\")\n",
    "            print(f\"Reference: {test_sentences[i][1]}\")\n",
    "            print(f\"Generated: {translated_sentences[i]}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate IBM Model 1 usage with evaluation.\"\"\"\n",
    "    print(\"Initializing IBM Model 1...\")\n",
    "    model = IBMModel1()\n",
    "    \n",
    "    print(f\"Italian Vocabulary Size: {len(model.italian_vocab)}\")\n",
    "    print(f\"English Vocabulary Size: {len(model.english_vocab)}\")\n",
    "    print(f\"Translation Probabilities Shape: {len(model.translation_probabilities)} x {len(model.translation_probabilities[0])}\")\n",
    "    \n",
    "    # Train the model with timing information\n",
    "    # Set show_times=True to see detailed timing information\n",
    "    model.train()\n",
    "    \n",
    "    # Prepare test data (use remaining data for testing)\n",
    "    training_size = int(model.TRAINING_DATA_PERCENTAGE * len(model.dataset))\n",
    "    test_data = model.dataset[training_size:training_size + 5]  # Use 500 sentences for testing\n",
    "    \n",
    "    if test_data:\n",
    "        print(f\"\\nEvaluating model on {len(test_data)} test sentences...\")\n",
    "        evaluation_results = model.evaluate_bleu(test_data)\n",
    "        \n",
    "        # Save evaluation results\n",
    "        print(\"\\nEvaluation complete!\")\n",
    "        print(f\"Main BLEU-4 Score: {evaluation_results['corpus_bleu_4']:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo test data available for evaluation.\")\n",
    "    \n",
    "    # Demonstrate individual word translation\n",
    "    print(\"\\nExample word translations:\")\n",
    "    test_words = [\"hello\", \"the\", \"house\", \"good\", \"time\"]\n",
    "    for word in test_words:\n",
    "        if word in model.english_mapping:\n",
    "            best_translations = model.get_best_translations(word, top_k=3)\n",
    "            print(f\"{word} -> {best_translations}\")\n",
    "    \n",
    "    # Demonstrate sentence translation\n",
    "    print(\"\\nExample sentence translations:\")\n",
    "    test_sentences = [\n",
    "        \"the house is good\",\n",
    "        \"hello my friend\",\n",
    "        \"this is a test\"\n",
    "    ]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        translation = model.translate(sentence)\n",
    "        print(f\"EN: {sentence}\")\n",
    "        print(f\"IT: {translation}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba73d0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8062b9f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42f6e7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mIBMModel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model.train()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mIBMModel1.__init__\u001b[39m\u001b[34m(self, seed)\u001b[39m\n\u001b[32m     39\u001b[39m random.seed(seed)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Load and prepare dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = \u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m random.shuffle(\u001b[38;5;28mself\u001b[39m.dataset)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Initialize vocabularies and mappings\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mread_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     29\u001b[39m             \u001b[38;5;66;03m# Strip whitespace and split by tab\u001b[39;00m\n\u001b[32m     30\u001b[39m             parts = line.strip().split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m             dataset.append((\u001b[43mclean_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, clean_sentence(parts[\u001b[32m1\u001b[39m])))\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sentence pairs from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mclean_sentence\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_sentence\u001b[39m(sentence : \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      7\u001b[39m     mapping = \u001b[38;5;28mstr\u001b[39m.maketrans(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.!\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m,?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msentence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = IBMModel1()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a9b10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f34dd2",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a032b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m training_size = \u001b[38;5;28mint\u001b[39m(\u001b[43mmodel\u001b[49m.TRAINING_DATA_PERCENTAGE * \u001b[38;5;28mlen\u001b[39m(model.dataset))\n\u001b[32m      2\u001b[39m test_data = model.dataset[training_size:]\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "training_size = int(model.TRAINING_DATA_PERCENTAGE * len(model.dataset))\n",
    "test_data = model.dataset[training_size:]\n",
    "evaluation_results = model.evaluate_bleu(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8efc0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
